services:
  ollama:
    container_name: ollama
    restart: unless-stopped
    image: ollama/ollama:latest
    runtime: nvidia
    network_mode: "host"
    environment:
      - NVIDIA_VISIBLE_DEVICES=all
    volumes:
      - "./ollama:/root/.ollama"
    ports:
      - 11434:11434
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all  # Use all available GPUs
              capabilities: [gpu]
    healthcheck:
      test: ollama list || exit 1
      interval: 10s
      timeout: 30s
      retries: 5
      start_period: 10s
    command: >
      ollama pull qwen2.5:32b && ollama serve

  chatbot:
    build: .
    container_name: chatbot
    ports:
      - "50001:50001"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all  # Use all available GPUs
              capabilities: [gpu]
    network_mode: "host"
    environment:
      - OLLAMA_BASE_URL=http://ollama:11434
      - CUDA_VISIBLE_DEVICES=all
    depends_on:
      - ollama
    runtime: nvidia