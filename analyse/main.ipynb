{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from typing import List\n",
    "from lib.train import Train\n",
    "from lib.youtube import YouTube\n",
    "from lib.constants import DEM_KEYWORDS, HSE_KEYWORDS, BASE_DATA_PATH\n",
    "from lib.utils import preprocess_text, fix_punctuations, count_tokens\n",
    "from tqdm.notebook import tqdm_notebook\n",
    "from tqdm import tqdm\n",
    "from lib.classify import classifier\n",
    "from lib.createdb import VectorDB\n",
    "from langdetect import detect\n",
    "from langchain_qdrant import QdrantVectorStore, RetrievalMode\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from pprint import pprint\n",
    "from dotenv import load_dotenv\n",
    "from googleapiclient.discovery import build\n",
    "from setfit import SetFitModel\n",
    "import evaluate\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    classification_report\n",
    ")\n",
    "\n",
    "load_dotenv(dotenv_path=\".env\")\n",
    "API_KEY = os.getenv(\"YOUTUBE_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract relevant video data from YouTube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YT = YouTube(keywords=HSE_KEYWORDS, filename=\"youtube_HSE\")\n",
    "YT.get_transcript()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/youtube_HSE.csv\")\n",
    "tqdm.pandas()\n",
    "\n",
    "# Detect the language\n",
    "df['lang'] = df['transcript'].progress_apply(lambda x: detect(x))\n",
    "df = df[df['lang'] == 'en']\n",
    "\n",
    "# Preprocess\n",
    "df['transcript'] = df['transcript'].progress_apply(preprocess_text)\n",
    "df['transcript'] = df['transcript'].progress_apply(fix_punctuations)\n",
    "\n",
    "df.to_csv(os.path.join(BASE_DATA_PATH, 'youtube_hse_v1.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e7939e5daa7d4ea1a95a84a8dde2d45a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1301 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "is_related\n",
       "yes    863\n",
       "no     438\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(BASE_DATA_PATH, 'youtube_hse_v1.csv'))\n",
    "\n",
    "decisions = []\n",
    "for index, row in tqdm_notebook(df.iterrows(), total=df.shape[0]):\n",
    "    try:\n",
    "        result = classifier(\n",
    "            text=row['transcript'],\n",
    "            context=\"Health, Safety and Environment (HSE)\")\n",
    "        decisions.append(result['output'])\n",
    "    except Exception as e:\n",
    "        print(f\"Issue in index: {index}\")\n",
    "        print(e)\n",
    "        decisions.append('N/A')\n",
    "        pass\n",
    "\n",
    "df['is_related'] = decisions\n",
    "df['is_related'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "condition = df['is_related'] == 'yes'\n",
    "df_new = df[condition]\n",
    "df_new.to_csv(\n",
    "    os.path.join(BASE_DATA_PATH, 'youtube_hse_v2.csv'), \n",
    "    index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 863/863 [00:00<00:00, 80419.56it/s]\n",
      "Processing Documents: 100%|██████████| 863/863 [10:52:38<00:00, 45.37s/it]   \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(BASE_DATA_PATH, 'youtube_hse_v2.csv'))\n",
    "VDB = VectorDB(model=\"llama3.3:latest\", dataframe=df)\n",
    "chunks = VDB.create_chunks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>video_id</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>page_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yYsDLBAp6LM</td>\n",
       "      <td>Emergency Evacuation Planning</td>\n",
       "      <td>https://www.youtube.com/watch?v=yYsDLBAp6LM</td>\n",
       "      <td>in case of emergency, personnel and the local ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>yYsDLBAp6LM</td>\n",
       "      <td>Emergency Evacuation Planning</td>\n",
       "      <td>https://www.youtube.com/watch?v=yYsDLBAp6LM</td>\n",
       "      <td>we are using the latest location technology to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aicxqzb3DJ4</td>\n",
       "      <td>Emergency Evacuations: Planning for the Whole ...</td>\n",
       "      <td>https://www.youtube.com/watch?v=aicxqzb3DJ4</td>\n",
       "      <td>- [Paul]. Hello everyone and thank you for joi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>aicxqzb3DJ4</td>\n",
       "      <td>Emergency Evacuations: Planning for the Whole ...</td>\n",
       "      <td>https://www.youtube.com/watch?v=aicxqzb3DJ4</td>\n",
       "      <td>Madeline over to you. - [Madeline]. Thank you,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aicxqzb3DJ4</td>\n",
       "      <td>Emergency Evacuations: Planning for the Whole ...</td>\n",
       "      <td>https://www.youtube.com/watch?v=aicxqzb3DJ4</td>\n",
       "      <td>So in the remainder of this first presentation...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8848</th>\n",
       "      <td>HpcFczHymBE</td>\n",
       "      <td>Disaster, Conflict, and Impact Assessment  Mak...</td>\n",
       "      <td>https://www.youtube.com/watch?v=HpcFczHymBE</td>\n",
       "      <td>I think the first point could be debated. I've...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8849</th>\n",
       "      <td>HpcFczHymBE</td>\n",
       "      <td>Disaster, Conflict, and Impact Assessment  Mak...</td>\n",
       "      <td>https://www.youtube.com/watch?v=HpcFczHymBE</td>\n",
       "      <td>they're engaged in one side or the other of th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8850</th>\n",
       "      <td>HpcFczHymBE</td>\n",
       "      <td>Disaster, Conflict, and Impact Assessment  Mak...</td>\n",
       "      <td>https://www.youtube.com/watch?v=HpcFczHymBE</td>\n",
       "      <td>and if so, what advice would you have for this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8851</th>\n",
       "      <td>HpcFczHymBE</td>\n",
       "      <td>Disaster, Conflict, and Impact Assessment  Mak...</td>\n",
       "      <td>https://www.youtube.com/watch?v=HpcFczHymBE</td>\n",
       "      <td>you want to reduce risk, make it safer than be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8852</th>\n",
       "      <td>HpcFczHymBE</td>\n",
       "      <td>Disaster, Conflict, and Impact Assessment  Mak...</td>\n",
       "      <td>https://www.youtube.com/watch?v=HpcFczHymBE</td>\n",
       "      <td>etc. I think that to a certain degree, the bas...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8853 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         video_id                                              title  \\\n",
       "0     yYsDLBAp6LM                      Emergency Evacuation Planning   \n",
       "1     yYsDLBAp6LM                      Emergency Evacuation Planning   \n",
       "2     aicxqzb3DJ4  Emergency Evacuations: Planning for the Whole ...   \n",
       "3     aicxqzb3DJ4  Emergency Evacuations: Planning for the Whole ...   \n",
       "4     aicxqzb3DJ4  Emergency Evacuations: Planning for the Whole ...   \n",
       "...           ...                                                ...   \n",
       "8848  HpcFczHymBE  Disaster, Conflict, and Impact Assessment  Mak...   \n",
       "8849  HpcFczHymBE  Disaster, Conflict, and Impact Assessment  Mak...   \n",
       "8850  HpcFczHymBE  Disaster, Conflict, and Impact Assessment  Mak...   \n",
       "8851  HpcFczHymBE  Disaster, Conflict, and Impact Assessment  Mak...   \n",
       "8852  HpcFczHymBE  Disaster, Conflict, and Impact Assessment  Mak...   \n",
       "\n",
       "                                              url  \\\n",
       "0     https://www.youtube.com/watch?v=yYsDLBAp6LM   \n",
       "1     https://www.youtube.com/watch?v=yYsDLBAp6LM   \n",
       "2     https://www.youtube.com/watch?v=aicxqzb3DJ4   \n",
       "3     https://www.youtube.com/watch?v=aicxqzb3DJ4   \n",
       "4     https://www.youtube.com/watch?v=aicxqzb3DJ4   \n",
       "...                                           ...   \n",
       "8848  https://www.youtube.com/watch?v=HpcFczHymBE   \n",
       "8849  https://www.youtube.com/watch?v=HpcFczHymBE   \n",
       "8850  https://www.youtube.com/watch?v=HpcFczHymBE   \n",
       "8851  https://www.youtube.com/watch?v=HpcFczHymBE   \n",
       "8852  https://www.youtube.com/watch?v=HpcFczHymBE   \n",
       "\n",
       "                                           page_content  \n",
       "0     in case of emergency, personnel and the local ...  \n",
       "1     we are using the latest location technology to...  \n",
       "2     - [Paul]. Hello everyone and thank you for joi...  \n",
       "3     Madeline over to you. - [Madeline]. Thank you,...  \n",
       "4     So in the remainder of this first presentation...  \n",
       "...                                                 ...  \n",
       "8848  I think the first point could be debated. I've...  \n",
       "8849  they're engaged in one side or the other of th...  \n",
       "8850  and if so, what advice would you have for this...  \n",
       "8851  you want to reduce risk, make it safer than be...  \n",
       "8852  etc. I think that to a certain degree, the bas...  \n",
       "\n",
       "[8853 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [{**doc.metadata, 'page_content': doc.page_content} for doc in chunks]\n",
    "docs_df = pd.DataFrame(data)\n",
    "docs_df.to_csv(os.path.join(BASE_DATA_PATH, \"yt_chunks_llama3.3-70b-hse.csv\"), index=False)\n",
    "docs_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a90c72bf5184ebaba60bd93035a302d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.read_csv(os.path.join(BASE_DATA_PATH, \"yt_chunks_llama3.3-70b-dem.csv\"))\n",
    "# df2 = pd.read_csv(os.path.join(BASE_DATA_PATH, \"yt_chunks_qwen2.5-32b-hse.csv\"))\n",
    "\n",
    "# Concat two dataframes\n",
    "# df = pd.concat([df1, df2])\n",
    "\n",
    "\n",
    "lengths = count_tokens(df)\n",
    "df['token_length'] = lengths\n",
    "df = df[df['token_length'] >= 100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DB = VectorDB(model=\"qwen2.5:32b\", dataframe=df)\n",
    "documents = DB.get_documents()\n",
    "DB.create_vectordb(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a vector database snapshot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'video_id': 'pGJliOHx1cc', 'title': 'Intelligence-enabled work health and safety, Maureen Hassall', 'url': 'https://www.youtube.com/watch?v=pGJliOHx1cc', '_id': 'fb28ec60-7414-4366-acf8-6f26ad6d420b', '_collection_name': 'youtube_collection'}, page_content=\"- Well, it's actually happening with the guy in the field. - [Chris], Yeah - So, wearing some of the tech, or with the cameras, with the mobile devices, they are filming and talking, They are looking at situations in the field, or they're looking at a piece of equipment, for example, in the field, And they're beaming the vision and having the conversation back to the manufacturer, which could be anywhere in the world, or back to the, say, the expert engineers who are sitting in a corporate office somewhere because they can't fly it in the moment. That's happening now?\"),\n",
      " Document(metadata={'video_id': 'pfAjb2gCvJs', 'title': 'Safety Management System', 'url': 'https://www.youtube.com/watch?v=pfAjb2gCvJs', '_id': 'fcb44a38-266f-49bd-a6b5-402ded042b80', '_collection_name': 'youtube_collection'}, page_content='An emergency response plan (ERP) is an integral component of a organisation’s safety process to address aviation related emergencies, crises or events. How can the babysitter know lots of things about the baby’s safety? Caring for young children is not easy. When the parents leave, the babysitter is responsible for keeping that child safe, fed and entertained. If there has been no prior formal training or education, how can these babysitting skills be improved? By reading books on the subject, Through online research and reading articles on the internet? In aviation, we can call this resource of information “documentation”.'),\n",
      " Document(metadata={'video_id': 'iTZ2N-HJbwA', 'title': 'How a Brain Implant and AI Gave a Woman with Paralysis Her Voice Back', 'url': 'https://www.youtube.com/watch?v=iTZ2N-HJbwA', '_id': '53027b73-08c3-43e9-af7a-b72a88958ce1', '_collection_name': 'youtube_collection'}, page_content=\"[Ann] What time will you be home? [Bill] In about an hour. [Ann], Do not make me laugh. [Bill] That's the first time we've ever had a conversation using this system. [Margaret] In order to communicate in her day-to-day life, she has an assistive communication device. These are dollar store glasses And she interfaces with it using a little reflective sticker on them. It's very slow and the device now is very old, but she relies on it. [Bill] For us to have that conversation using her Dynavox would probably be like a 5-to-7-minute conversation. [Ann] It was nice to have a conversation. I forget how slow this machine is. [Margaret] When Ann was 30 years old, she was playing volleyball with some friends and had a stroke which led to the condition that she has now, which is locked-in syndrome. At this point she had a 6-month-old child and a 7-year-old child. [Ann] You are truly wonderful people. [Margaret] The physicians have no idea why this happened. That was now 18 years ago. How can we create technology that can help people really meaningfully contribute with all sorts of abilities? [Ann] Hi, How are things going?\"),\n",
      " Document(metadata={'video_id': '1uMbRimBAO8', 'title': 'Improving construction worker safety with wearable sensors', 'url': 'https://www.youtube.com/watch?v=1uMbRimBAO8', '_id': 'd5fa5e34-7f44-4421-8747-e693164d1ac5', '_collection_name': 'youtube_collection'}, page_content=\"Narrator. Professor of Civil and Environmental Engineering Sanghyun Lee and his team of University of Michigan researchers are placing inexpensive wearable sensors on construction workers to measure anxiety, fatigue and heat stress while on the job. >> Gaang Lee. I worked at construction company for five years. Our research can help them to be safe. [construction site sounds]. Per day we will check their perceived heat stress and perceived fatigue four times. >> Narrator. The sensors will read body movement, temperature and electrodermal responses, while saliva samples verify some of the incoming data. The goal is to use the sensors to establish a model that will help predict when workers will become dehydrated, anxious or when accidental falls are most likely to occur. >> Harris. We're seeing that some of the guys are staying in the field longer. You know they're wearing their bodies out, given the type of work that they do and as we move along through this project, I wouldn't be surprised, five, ten years down the road, that some of the research that these guys are doing here today has changes in the way that we do things in the field in the future. [music].\")]\n"
     ]
    }
   ],
   "source": [
    "vector_store = QdrantVectorStore.from_existing_collection(\n",
    "    embedding=OllamaEmbeddings(model=\"qwen2.5:32b\"),\n",
    "    collection_name=\"youtube_collection\",\n",
    "    url=\"http://localhost:6333\",\n",
    "    retrieval_mode=RetrievalMode.DENSE,\n",
    ")\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "result = retriever.invoke(\"What are the trending AI technologies in Disaster Management?\")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Thumbnails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a6111e84346401e9aebb84057f0aed3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching thumbnails:   0%|          | 0/863 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# df = pd.read_csv(os.path.join(BASE_DATA_PATH, 'youtube_hse_v2.csv'))\n",
    "\n",
    "# def search_youtube(dataframe: pd.DataFrame) -> List[str]:\n",
    "#         urls = []\n",
    "#         youtube = build('youtube', 'v3', developerKey=API_KEY)\n",
    "#         for index, row in tqdm_notebook(dataframe.iterrows(),\n",
    "#                                         total=dataframe.shape[0],\n",
    "#                                         desc=\"Fetching thumbnails\"):\n",
    "#             request = youtube.videos().list(\n",
    "#                 part='snippet',\n",
    "#                 id=row['video_id'],\n",
    "#             )\n",
    "#             response = request.execute()\n",
    "#             urls.append(response[\"items\"][0][\"snippet\"]['thumbnails']['default']['url'])\n",
    "#         return urls\n",
    "\n",
    "# urls = search_youtube(df)\n",
    "# df['thumbnails'] = urls\n",
    "# df.to_csv(os.path.join(BASE_DATA_PATH, 'youtube_hse_v2.csv'), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(os.path.join(BASE_DATA_PATH, \"yt_chunks_qwen2.5-32b-hse.csv\"))\n",
    "df2 = pd.read_csv(os.path.join(BASE_DATA_PATH, 'youtube_hse_v2.csv'))\n",
    "\n",
    "\n",
    "df1 = df1.merge(df2[['video_id', 'thumbnails']], on='video_id', how='left')\n",
    "df1\n",
    "df1.to_csv(os.path.join(BASE_DATA_PATH, \"yt_chunks_qwen2.5-32b-hse.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-Tune Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "model_head.pkl not found on HuggingFace Hub, initialising classification head with random weights. You should TRAIN this model on a downstream task to use it for predictions and inference.\n",
      "Currently using DataParallel (DP) for multi-gpu training, while DistributedDataParallel (DDP) is recommended for faster training. See https://sbert.net/docs/sentence_transformer/training/distributed.html for more information.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cc18d5a018746e3959e789c73c57a31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/160 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num unique pairs = 12880\n",
      "  Batch size = 16\n",
      "  Num epochs = 1\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='202' max='202' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [202/202 00:27, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.220700</td>\n",
       "      <td>0.177177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.166200</td>\n",
       "      <td>0.106601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.082700</td>\n",
       "      <td>0.081038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.038700</td>\n",
       "      <td>0.078453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.019200</td>\n",
       "      <td>0.074794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.009800</td>\n",
       "      <td>0.077936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.006400</td>\n",
       "      <td>0.079420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.005400</td>\n",
       "      <td>0.080536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.005500</td>\n",
       "      <td>0.082035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.082227</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee2012d9a9ae4fb5a45d3c8098766207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing widget examples:   0%|          | 0/1 [00:00<?, ?example/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.95,\n",
      " 'f1': 0.9523809523809523,\n",
      " 'precision': 0.9090909090909091,\n",
      " 'recall': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5543221be93402881a0ae71f6e52a74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24abd90c759549b2930de9e6bb6b7bca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c0f5d8b55f4bf2973413b71a098719",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_head.pkl:   0%|          | 0.00/3.94k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "RUNNER = Train(model_name='sentence-transformers/all-MiniLM-L6-v2',\n",
    "               dataset_path='data/train/setfit.csv')\n",
    "\n",
    "train, validation, test = RUNNER.split_data()\n",
    "RUNNER.train(train_dataset=train, eval_dataset=validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Test the trained model\n",
    "model = SetFitModel.from_pretrained('sharukat/adersim-dem-hse')\n",
    "preds = model.predict(test['text'])\n",
    "preds = preds.tolist() \n",
    "\n",
    "accuracy = accuracy_score(test['label'], preds)\n",
    "precision = precision_score(test['label'], preds, average=\"binary\")\n",
    "recall = recall_score(test['label'], preds, average=\"binary\")\n",
    "f1 = f1_score(test['label'], preds, average=\"binary\")\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        10\n",
      "           1       1.00      1.00      1.00        10\n",
      "\n",
      "    accuracy                           1.00        20\n",
      "   macro avg       1.00      1.00      1.00        20\n",
      "weighted avg       1.00      1.00      1.00        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(test['label'], preds))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adersim-app-2-analyse-14fX-SLK-py3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
